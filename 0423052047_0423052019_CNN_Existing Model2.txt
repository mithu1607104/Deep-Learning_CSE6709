import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load Dataset
file_path = '/content/drive/My Drive/BUET/Deep learning/Dataset_with_named_clusters.csv'  # Update the file path
df = pd.read_csv(file_path)

# Step 2: Preprocess the Sequence Data
# One-hot encode the sequence features
def one_hot_encode_sequence(sequences, max_length, vocab="ACDEFGHIKLMNPQRSTVWY"):
    vocab_dict = {char: idx for idx, char in enumerate(vocab)}
    encoded = np.zeros((len(sequences), max_length, len(vocab)))
    for i, seq in enumerate(sequences):
        for j, char in enumerate(seq[:max_length]):
            if char in vocab_dict:
                encoded[i, j, vocab_dict[char]] = 1
    return encoded

max_sequence_length = 60  # Maximum sequence length to use
vocab = "ACDEFGHIKLMNPQRSTVWY"  # Amino acid vocabulary
X_sequences = one_hot_encode_sequence(df['Sequence'].fillna(""), max_length=max_sequence_length, vocab=vocab)

# Step 3: Encode Target Variable
label_encoder = LabelEncoder()
y_labels = label_encoder.fit_transform(df['Cluster_Name'])
y_categorical = to_categorical(y_labels)  # Convert to one-hot encoding for CNN

# Step 4: Split Data into Train and Test Sets
X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_categorical, test_size=0.2, random_state=42)

# Step 5: Define the CNN Model
model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(max_sequence_length, len(vocab))),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=128, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(y_categorical.shape[1], activation='softmax')  # Output layer with softmax for multi-class classification
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Step 6: Train the CNN Model
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# Step 7: Evaluate the Model
y_pred = np.argmax(model.predict(X_test), axis=1)
y_test_labels = np.argmax(y_test, axis=1)

accuracy = accuracy_score(y_test_labels, y_pred)
print(f"Test Accuracy: {accuracy}")
print("\nClassification Report:")
print(classification_report(y_test_labels, y_pred, target_names=label_encoder.classes_))

# Step 8: Confusion Matrix
conf_matrix = confusion_matrix(y_test_labels, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

# Step 9: Plot Accuracy and Loss
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()




# With hypermeter tuning
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load Dataset
file_path = '/content/drive/My Drive/BUET/Deep learning/Dataset_with_named_clusters.csv'  # Update the file path
df = pd.read_csv(file_path)

# Step 2: Preprocess the Sequence Data
# One-hot encode the sequence features
def one_hot_encode_sequence(sequences, max_length, vocab="ACDEFGHIKLMNPQRSTVWY"):
    vocab_dict = {char: idx for idx, char in enumerate(vocab)}
    encoded = np.zeros((len(sequences), max_length, len(vocab)))
    for i, seq in enumerate(sequences):
        for j, char in enumerate(seq[:max_length]):
            if char in vocab_dict:
                encoded[i, j, vocab_dict[char]] = 1
    return encoded

max_sequence_length = 60  # Maximum sequence length to use
vocab = "ACDEFGHIKLMNPQRSTVWY"  # Amino acid vocabulary
X_sequences = one_hot_encode_sequence(df['Sequence'].fillna(""), max_length=max_sequence_length, vocab=vocab)
# Step 3: Encode Target Variable
label_encoder = LabelEncoder()
y_labels = label_encoder.fit_transform(df['Cluster_Name'])
y_categorical = to_categorical(y_labels)  # Convert to one-hot encoding for CNN
# Step 4: Split Data into Train and Test Sets
X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_categorical, test_size=0.2, random_state=42)
# Step 5: Define the CNN Model
model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(max_sequence_length, len(vocab))),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=64, kernel_size=7, activation='relu'),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(y_categorical.shape[1], activation='softmax')  # Output layer with softmax for multi-class classification
])
# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
# Step 6: Train the CNN Model
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))
# Step 7: Evaluate the Model
y_pred = np.argmax(model.predict(X_test), axis=1)
y_test_labels = np.argmax(y_test, axis=1)
accuracy = accuracy_score(y_test_labels, y_pred)
print(f"Test Accuracy: {accuracy}")
print("\nClassification Report:")
print(classification_report(y_test_labels, y_pred, target_names=label_encoder.classes_))
# Step 8: Confusion Matrix
conf_matrix = confusion_matrix(y_test_labels, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

# Step 9: Plot Accuracy and Loss
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.tight_layout()
plt.show()
