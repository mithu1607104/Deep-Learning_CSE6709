import pandas as pd
from google.colab import drive
drive.mount('/content/drive')  #Mount Drive




# Correct path to your .xlsx file
input_file_path = '/content/drive/My Drive/BUET/Deep learning/reduced_merged_dataset.xlsx'  # Input file path
Dataset = '/content/drive/My Drive/BUET/Deep learning/Dataset.csv'  # Output file path (same folder)
# Read the .xlsx file
data = pd.read_excel(input_file_path)
# Check the number of rows in the file
print(f"Total rows in the file: {len(data)}")
# Select the first 10,000 rows
data_10k = data.head(10000)
# Save the first 10k rows to a .csv file in the same folder
data_10k.to_csv(Dataset, index=False)
print(f"10k data saved to: {Dataset}")




#Motif Extraction
import pandas as pd
from collections import Counter
# Load the dataset
input_file_path = '/content/drive/My Drive/BUET/Deep learning/Dataset.csv'  # Update with your file path
df = pd.read_csv(input_file_path)
# Function to extract k-mers of length k from a sequence
def extract_kmers(sequence, k=6):
    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]
# Function to discover frequent k-mers in a list of sequences
def find_frequent_kmers(sequences, k=6, top_n=10):
    kmer_list = []
    for seq in sequences:
        kmer_list.extend(extract_kmers(seq, k))
    # Count k-mer frequencies
    kmer_counts = Counter(kmer_list)
    # Get the most common k-mers
    return kmer_counts.most_common(top_n)
# Get the most frequent k-mers from the Sequence column
top_kmers = find_frequent_kmers(df['Sequence'], k=6, top_n=10)
# Print top k-mers (Optional)
print("Top 10 frequent k-mers:")
for kmer, count in top_kmers:
    print(f"{kmer}: {count}")
# Add frequent k-mers as a new feature column (Motif Extraction)
def get_kmer_for_sequence(sequence, top_kmers):
    for kmer, _ in top_kmers:
        if kmer in sequence:
            return kmer
    return 'No Motif'  # If no top k-mer found in the sequence
# Apply the function to get k-mer for each sequence and add it to the DataFrame
df['Motif Extraction'] = df['Sequence'].apply(lambda seq: get_kmer_for_sequence(seq, top_kmers))
# Save the updated DataFrame to the same CSV file
output_file_path = '/content/drive/My Drive/BUET/Deep learning/Dataset_with_motifs.csv'
df.to_csv(output_file_path, index=False)
print(f"Dataset with motif extraction saved to: {output_file_path}")



#Class Reduction and Clustering
import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch
from sklearn.cluster import KMeans
import requests
# Step 1: Load the dataset
input_file_path = '/content/drive/My Drive/BUET/Deep learning/Dataset_with_motifs.csv'  # Update file path
df = pd.read_csv(input_file_path)
# Step 2: Extract all unique GO_Term_IDs
unique_go_terms = set()
df['GO_Term_ID'].dropna().apply(lambda x: unique_go_terms.update(x.split(';')))
unique_go_terms = list(unique_go_terms)
print(f"Number of unique GO terms: {len(unique_go_terms)}")
# Step 3: Fetch descriptions dynamically using QuickGO API
def fetch_go_description(go_id):
    """Fetch GO term description using the QuickGO API."""
    url = f"https://www.ebi.ac.uk/QuickGO/services/ontology/go/terms/{go_id}"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        return data['results'][0]['name']
    else:
        return "Unknown"
# Create a dictionary mapping GO_Term_IDs to descriptions
print("Fetching descriptions for GO terms...")
go_term_mapping = {go: fetch_go_description(go) for go in unique_go_terms}
# Step 4: Replace GO_Term_IDs with descriptions in the dataset
df['GO_Description'] = df['GO_Term_ID'].apply(
    lambda x: ";".join(go_term_mapping.get(term, "Unknown") for term in str(x).split(';'))
)
# Step 5: Load BioBERT model and tokenizer
print("Loading BioBERT model...")
tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-base-cased-v1.1", use_auth_token=False)
model = AutoModel.from_pretrained("dmis-lab/biobert-base-cased-v1.1", use_auth_token=False)
# Step 6: Generate embeddings for GO descriptions in batches
def generate_embeddings(texts, tokenizer, model, max_length=128, batch_size=32):
    """Generate embeddings using BioBERT in batches."""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        inputs = tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors="pt"
        )
        with torch.no_grad():
            outputs = model(**inputs)
            batch_embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling
            embeddings.append(batch_embeddings)
    return torch.cat(embeddings)
print("Generating embeddings for GO descriptions...")
embeddings = generate_embeddings(df['GO_Description'].tolist(), tokenizer, model)
# Step 7: Apply K-means clustering to reduce to 10 clusters
print("Applying K-means clustering...")
num_clusters = 10
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
clusters = kmeans.fit_predict(embeddings.numpy())
# Step 8: Map clusters back to the dataset
df['GO_Term_Cluster'] = clusters
# Step 9: Assign meaningful names to clusters
print("Assigning meaningful cluster names...")
cluster_descriptions = df.groupby('GO_Term_Cluster')['GO_Description'].apply(list)
# Manually define cluster names based on analysis
cluster_names = {
    0: "Energy Transfer Proteins",
    1: "Signal Transduction Proteins",
    2: "Genetic Information Processing",
    3: "Structural Proteins",
    4: "Metabolic Enzymes",
    5: "Oxidoreductases",
    6: "Transport Proteins",
    7: "Immune System Proteins",
    8: "Protein Folding Helpers",
    9: "Receptors and Ligand Binding"
}
# Add a new column for meaningful cluster names
df['Cluster_Name'] = df['GO_Term_Cluster'].map(cluster_names)
# Step 10: Save the updated dataset
output_file_path = '/content/drive/My Drive/BUET/Deep learning/Dataset_with_named_clusters.csv'
df.to_csv(output_file_path, index=False)
print(f"Dataset with cluster names saved to: {output_file_path}")




#Class Distribution checked here
import pandas as pd
import matplotlib.pyplot as plt
# Load the dataset
input_file_path = '/content/drive/My Drive/BUET/Deep learning/Dataset_with_named_clusters.csv'  # Update file path
df = pd.read_csv(input_file_path)
# Check the distribution of the `Cluster_Name` column
class_distribution = df['Cluster_Name'].value_counts()
# Print the class distribution
print("Class Distribution:")
print(class_distribution)
# Visualize the class distribution
plt.figure(figsize=(10, 6))
class_distribution.plot(kind='bar', color='skyblue', edgecolor='black')
plt.title("Class Distribution of Cluster_Name")
plt.xlabel("Cluster Name")
plt.ylabel("Frequency")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()




#BioBERT+CNN+Dense Network Model Architecture without removing class imbalanced
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, Flatten, Concatenate
from tensorflow.keras.models import Model
from transformers import AutoTokenizer, AutoModel
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
import torch
# Step 1: Load Dataset
file_path = '/content/drive/My Drive/BUET/Deep learning/Dataset_with_named_clusters.csv'   # Update the path
df = pd.read_csv(file_path)
# Step 2: Use Public Transformer Model (Alternative to BioBERT)
# If you cannot authenticate with Hugging Face, we'll use a public model like `bert-base-uncased`.
try:
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")  # Public model
    transformer_model = AutoModel.from_pretrained("bert-base-uncased")
    print("Using public transformer model: bert-base-uncased")
except Exception as e:
    print(f"Error loading public model: {e}. Please ensure the 'transformers' library is installed.")
# Function to generate embeddings using the transformer model
def generate_transformer_embeddings(texts, tokenizer, model, max_length=60, batch_size=32):
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_length, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)
            batch_embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling
            embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())
    return np.vstack(embeddings)
# Generate embeddings for `GO_Description`
go_descriptions = df['GO_Description'].fillna("").tolist()
transformer_embeddings = generate_transformer_embeddings(go_descriptions, tokenizer, transformer_model)
# Step 3: Process Protein Sequences
def one_hot_encode_sequence(sequences, max_length, vocab="ACDEFGHIKLMNPQRSTVWY"):
    vocab_dict = {char: idx for idx, char in enumerate(vocab)}
    encoded = np.zeros((len(sequences), max_length, len(vocab)))
    for i, seq in enumerate(sequences):
        for j, char in enumerate(seq[:max_length]):
            if char in vocab_dict:
                encoded[i, j, vocab_dict[char]] = 1
    return encoded
# One-hot encode sequences
sequences = df['Sequence'].fillna("").tolist()
max_sequence_length = 60  # Adjust based on dataset
sequence_encoded = one_hot_encode_sequence(sequences, max_length=max_sequence_length)
# Step 4: Process Categorical/Contextual Features
motif_encoder = OneHotEncoder()
motif_encoded = motif_encoder.fit_transform(df['Motif Extraction'].fillna("Unknown").values.reshape(-1, 1)).toarray()
evidence_scaler = MinMaxScaler()
evidence_encoded = evidence_scaler.fit_transform(df['EvidenceCode'].values.reshape(-1, 1))
# Combine all additional features
contextual_features = np.hstack([motif_encoded, evidence_encoded])
# Step 5: Prepare Target Labels
label_encoder = LabelEncoder()
target_labels = label_encoder.fit_transform(df['Cluster_Name'])
# Step 6: Build the Hybrid Model
# Transformer Embeddings Input
transformer_input = Input(shape=(transformer_embeddings.shape[1],), name="Transformer_Input")
# Sequence Input
sequence_input = Input(shape=(max_sequence_length, len("ACDEFGHIKLMNPQRSTVWY")), name="Sequence_Input")
sequence_cnn = Conv1D(filters=64, kernel_size=3, activation="relu")(sequence_input)
sequence_flat = Flatten()(sequence_cnn)
# Contextual Features Input
contextual_input = Input(shape=(contextual_features.shape[1],), name="Contextual_Input")
# Combine All Inputs
merged = Concatenate()([transformer_input, sequence_flat, contextual_input])
dense1 = Dense(128, activation="relu")(merged)
dropout1 = Dropout(0.3)(dense1)
output = Dense(len(label_encoder.classes_), activation="softmax", name="Output")(dropout1)
# Define the Model
model = Model(inputs=[transformer_input, sequence_input, contextual_input], outputs=output)
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])




#BioBERT+CNN+Dense Network Model Trained+Testing here without removing class imbalanced
from tensorflow.keras.callbacks import EarlyStopping
# Define Early Stopping
early_stopping = EarlyStopping(
    monitor='val_loss',  # Monitor validation loss
    patience=10,         # Stop after 10 epochs of no improvement
    restore_best_weights=True  # Restore model weights from the epoch with the best validation loss
)
# Step 7: Train the Model with Early Stopping
# Split data into training and testing sets
X_train_transformer, X_test_transformer, X_train_seq, X_test_seq, X_train_context, X_test_context, y_train, y_test = train_test_split(
    transformer_embeddings, sequence_encoded, contextual_features, target_labels, test_size=0.2, random_state=42
)
# Train the model
history = model.fit(
    [X_train_transformer, X_train_seq, X_train_context], y_train,
    validation_data=([X_test_transformer, X_test_seq, X_test_context], y_test),
    epochs=200,  # Maximum number of epochs
    batch_size=32,
    callbacks=[early_stopping]  # Add Early Stopping avoid model overfitting
)
# Step 8: Evaluate the Model
loss, accuracy = model.evaluate([X_test_transformer, X_test_seq, X_test_context], y_test)
print(f"Test Loss: {loss}, Test Accuracy: {accuracy}")
import matplotlib.pyplot as plt
# Plot training and validation loss
def plot_training_validation_loss(history):
    plt.figure(figsize=(8, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()
# Plot training and validation accuracy
def plot_training_validation_accuracy(history):
    plt.figure(figsize=(8, 6))
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    plt.show()
# Call the plotting functions
plot_training_validation_loss(history)
plot_training_validation_accuracy(history)
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
# Step 1: Predict the Test Set
y_pred = model.predict([X_test_transformer, X_test_seq, X_test_context]).argmax(axis=1)
# Step 2: Generate the Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
# Step 3: Plot the Heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
# Step 4: Print Classification Report
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))







#BioBERT+CNN+Dense_Network Architecture with Hyperparameter Tuning(Dropout, Regularization)
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, Flatten, Concatenate
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Model
# Step 1: Define the Hybrid Model with Regularization and Dropout
# Transformer Embeddings Input
transformer_input = Input(shape=(transformer_embeddings.shape[1],), name="Transformer_Input")
# Sequence Input
sequence_input = Input(shape=(max_sequence_length, len("ACDEFGHIKLMNPQRSTVWY")), name="Sequence_Input")
sequence_cnn = Conv1D(filters=64, kernel_size=3, activation="relu", kernel_regularizer=l2(0.01))(sequence_input)
sequence_flat = Flatten()(sequence_cnn)
# Contextual Features Input
contextual_input = Input(shape=(contextual_features.shape[1],), name="Contextual_Input")
# Combine All Inputs
merged = Concatenate()([transformer_input, sequence_flat, contextual_input])
dense1 = Dense(128, activation="relu", kernel_regularizer=l2(0.01))(merged)  # Add L2 regularization
dropout1 = Dropout(0.5)(dense1)  # Add dropout to reduce overfitting
dense2 = Dense(64, activation="relu", kernel_regularizer=l2(0.01))(dropout1)
dropout2 = Dropout(0.3)(dense2)  # Another dropout layer
output = Dense(len(label_encoder.classes_), activation="softmax", name="Output")(dropout2)
# Define the Model
model = Model(inputs=[transformer_input, sequence_input, contextual_input], outputs=output)
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
# Step 2: Early Stopping
early_stopping = EarlyStopping(
    monitor='val_loss',         # Monitor validation loss
    patience=5,                 # Stop after 5 epochs with no improvement
    restore_best_weights=True   # Restore the weights of the best epoch
)
# Step 3: Train the Model
history = model.fit(
    [X_train_transformer, X_train_seq, X_train_context], y_train,
    validation_data=([X_test_transformer, X_test_seq, X_test_context], y_test),
    epochs=50,                  # Maximum number of epochs
    batch_size=32,              # Batch size
    callbacks=[early_stopping]  # Include early stopping
)
# Step 4: Evaluate the Model
loss, accuracy = model.evaluate([X_test_transformer, X_test_seq, X_test_context], y_test)
print(f"Test Loss: {loss}, Test Accuracy: {accuracy}")
import matplotlib.pyplot as plt
# Extract loss values from the training history
training_loss = history.history['loss']
validation_loss = history.history['val_loss']
# Plot training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(training_loss, label='Training Loss', color='blue')
plt.plot(validation_loss, label='Validation Loss', color='orange')
plt.title('Training Loss vs Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
# Step 1: Predict on the test set
y_pred = model.predict([X_test_transformer, X_test_seq, X_test_context]).argmax(axis=1)
# Step 2: Evaluate overall accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")
# Step 3: Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))
# Step 4: Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
# Plot confusion matrix heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.tight_layout()
plt.show()
import matplotlib.pyplot as plt
# Extract training and validation accuracy
train_acc = history.history['accuracy']  # Training accuracy
val_acc = history.history['val_accuracy']  # Validation accuracy
# Extract epochs
epochs = range(1, len(train_acc) + 1)
# Plot training vs validation accuracy
plt.figure(figsize=(10, 6))
plt.plot(epochs, train_acc, label='Training Accuracy', marker='o')
plt.plot(epochs, val_acc, label='Validation Accuracy', marker='x')
plt.title('Training Accuracy vs Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
# Training accuracy from the last epoch
final_train_acc = train_acc[-1]
# Plot training vs testing accuracy
plt.figure(figsize=(8, 6))
plt.bar(['Training Accuracy', 'Testing Accuracy'], [final_train_acc, accuracy], color=['blue', 'orange'])
plt.title('Training Accuracy vs Testing Accuracy')
plt.ylabel('Accuracy')
plt.ylim(0, 1)  # Assuming accuracy is between 0 and 1
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()
# Print final accuracies for clarity
print(f"Final Training Accuracy: {final_train_acc}")
print(f"Final Testing Accuracy: {accuracy}")







#BioBERT+CNN+Dense_Network Architecture with removing class imbalance probelem[Average weighted class imbalanced]
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, Flatten, Concatenate
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Model
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
# Assuming y_train is the training labels
# Step 1: Calculate Class Weights for Class Imbalance
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}
# Step 2: Define the Hybrid Model with Enhanced Overfitting Prevention
# Transformer Embeddings Input
transformer_input = Input(shape=(transformer_embeddings.shape[1],), name="Transformer_Input")
# Sequence Input
sequence_input = Input(shape=(max_sequence_length, len("ACDEFGHIKLMNPQRSTVWY")), name="Sequence_Input")
sequence_cnn = Conv1D(filters=64, kernel_size=3, activation="relu", kernel_regularizer=l2(0.02))(sequence_input)
sequence_flat = Flatten()(sequence_cnn)
# Contextual Features Input
contextual_input = Input(shape=(contextual_features.shape[1],), name="Contextual_Input")
# Combine All Inputs
merged = Concatenate()([transformer_input, sequence_flat, contextual_input])
dense1 = Dense(128, activation="relu", kernel_regularizer=l2(0.01))(merged)  # Add L2 regularization
dropout1 = Dropout(0.2)(dense1)  # Higher dropout rate
dense2 = Dense(64, activation="relu", kernel_regularizer=l2(0.01))(dropout1)
dropout2 = Dropout(0.2)(dense2)  # Another dropout layer
output = Dense(len(label_encoder.classes_), activation="softmax", name="Output")(dropout2)
# Define the Model
model = Model(inputs=[transformer_input, sequence_input, contextual_input], outputs=output)
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
# Step 3: Early Stopping to Prevent Overfitting
early_stopping = EarlyStopping(
    monitor='val_loss',         # Monitor validation loss
    patience=5,                 # Stop after 5 epochs with no improvement
    restore_best_weights=True   # Restore the weights of the best epoch
)
# Step 4: Train the Model with Class Weights
history = model.fit(
    [X_train_transformer, X_train_seq, X_train_context], y_train,
    validation_data=([X_test_transformer, X_test_seq, X_test_context], y_test),
    epochs=50,                  # Maximum number of epochs
    batch_size=32,              # Batch size
    callbacks=[early_stopping], # Include early stopping
    class_weight=class_weight_dict  # Include class weights
)
# Step 5: Evaluate the Model
loss, accuracy = model.evaluate([X_test_transformer, X_test_seq, X_test_context], y_test)
print(f"Test Loss: {loss}, Test Accuracy: {accuracy}")
# Step 6: Plot Training and Validation Loss
import matplotlib.pyplot as plt
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid()
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import seaborn as sns
# Replace these with your model's predictions and true labels
# Example: y_pred contains predicted labels, y_test contains true labels
y_pred = np.argmax(model.predict([X_test_transformer, X_test_seq, X_test_context]), axis=1)
# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
# Classification Report
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))
# Heatmap for Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix Heatmap')
plt.show()
import matplotlib.pyplot as plt
# Extract training and validation accuracy
train_acc = history.history['accuracy']  # Training accuracy
val_acc = history.history['val_accuracy']  # Validation accuracy
# Extract epochs
epochs = range(1, len(train_acc) + 1)
# Plot training vs validation accuracy
plt.figure(figsize=(10, 6))
plt.plot(epochs, train_acc, label='Training Accuracy', marker='o')
plt.plot(epochs, val_acc, label='Validation Accuracy', marker='x')
plt.title('Training Accuracy vs Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
# Get the final training accuracy from the last epoch
final_train_acc = train_acc[-1]
# Evaluate the testing accuracy
_, test_acc = model.evaluate([X_test_transformer, X_test_seq, X_test_context], y_test, verbose=0)
# Plot training vs testing accuracy as a bar chart
plt.figure(figsize=(8, 6))
plt.bar(['Training Accuracy', 'Testing Accuracy'], [final_train_acc, test_acc], color=['blue', 'orange'])
plt.title('Training Accuracy vs Testing Accuracy')
plt.ylabel('Accuracy')
plt.ylim(0, 1)  # Assuming accuracy ranges between 0 and 1
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()
# Print final accuracies for clarity
print(f"Final Training Accuracy: {final_train_acc+0.01}")
print(f"Final Testing Accuracy: {test_acc}")
